{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9228c190-3b24-4a63-a486-670239258cf6",
   "metadata": {},
   "source": [
    "# Assignment 2 - Deep Q-Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a03902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "from pprint import pprint\n",
    "from gym import spaces\n",
    "from collections import OrderedDict, deque\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "from operator import add\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5274ebe5-8024-4614-a18c-3bfeccfac098",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyglet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4c56ed-b14d-4320-ac7f-37ce344b5c01",
   "metadata": {},
   "source": [
    "## 2. Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2d8e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNnet(nn.Module):\n",
    "    '''\n",
    "    DQNnet is a class that defines the Q-net\n",
    "    '''\n",
    "    def __init__(self, input_size, output_size, lr):\n",
    "        super(DQNnet, self).__init__()\n",
    "        self.hl1_size = 100\n",
    "        self.hl2_size = 50\n",
    "        self.linear_model = nn.Sequential(OrderedDict([\n",
    "            ('dense1', nn.Linear(input_size, self.hl1_size)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('dense2', nn.Linear(self.hl1_size, self.hl2_size)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('dense3', nn.Linear(self.hl2_size, output_size))]\n",
    "        ))\n",
    "        self.optimizer = optim.SGD(self.parameters(), lr=lr)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Perform forward pass in the network\n",
    "        '''\n",
    "        x = self.linear_model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f38bb-fb24-4c68-b3eb-2d68dd5cc30f",
   "metadata": {},
   "source": [
    "## 3. DQN-Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b241fc-527c-4c5f-a410-bd9742d1cd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNagent:\n",
    "    '''\n",
    "    DQNagent is a class that defines the agent.\n",
    "    '''\n",
    "    def __init__(self, state_size, action_size, epsilon, epsilon_decay, batch_size, lr, discount_factor, memory_len):\n",
    "        '''\n",
    "        Initialize all the parameters that will be used in the learning phase.\n",
    "        Initital two DQN networks - one is the policy network and the other is the target network\n",
    "        '''\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.memory = deque(maxlen=memory_len)\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = 0.001\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = lr\n",
    "        self.discount_factor = discount_factor\n",
    "        \n",
    "        self.device = 'cpu'\n",
    "        \n",
    "        self.policy_net = DQNnet(self.state_size, self.action_size, self.learning_rate).to(self.device)\n",
    "        self.target_net = DQNnet(self.state_size, self.action_size, self.learning_rate).to(self.device)\n",
    "        \n",
    "    def get_action(self, current_state_vector):\n",
    "        '''\n",
    "        This function defines epsilon greedy behavior of the agent.\n",
    "        '''\n",
    "        rand_num = np.random.random()\n",
    "        if rand_num < self.epsilon:\n",
    "            return np.random.randint(self.action_size)\n",
    "        else:\n",
    "            if not torch.is_tensor(current_state_vector):\n",
    "                current_state_vector = torch.from_numpy(current_state_vector).float()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                q_values = self.policy_net.forward(current_state_vector).view(-1,)\n",
    "            best_action = torch.argmax(q_values).item()\n",
    "        return best_action\n",
    "    \n",
    "    def append_memory(self, state, action, reward, next_state, done):\n",
    "        '''\n",
    "        Save each experience in the replay memory\n",
    "        '''\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def update_target_policy(self):\n",
    "        '''\n",
    "        Update the target network parameters based on the policy network parameters\n",
    "        '''\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        '''\n",
    "        Perform epsilon decay\n",
    "        '''\n",
    "        self.epsilon -= (self.epsilon * self.epsilon_decay)\n",
    "        return self.epsilon\n",
    "    \n",
    "    def train_model(self):\n",
    "        '''\n",
    "        Train the agent\n",
    "        '''\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            mini_batch = random.sample(self.memory, self.batch_size)\n",
    "            current_state = list()\n",
    "            actions = list()\n",
    "            rewards = list()\n",
    "            next_states = list()\n",
    "            dones = list()\n",
    "            \n",
    "            # Append each memory values in the separate lists\n",
    "            for i in range(self.batch_size):\n",
    "                state, action, reward, next_state, done_boolean = mini_batch[i]\n",
    "                current_state.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                next_states.append(next_state)\n",
    "                dones.append(done_boolean)\n",
    "            \n",
    "            # Convert each list to torch tensors\n",
    "            current_state = torch.from_numpy(np.array(current_state)).float().to(self.device)\n",
    "            actions = torch.from_numpy(np.array(actions)).to(self.device)\n",
    "            rewards = torch.from_numpy(np.array(rewards)).float().to(self.device)\n",
    "            next_states = torch.from_numpy(np.array(next_states)).float().to(self.device)\n",
    "            dones = torch.from_numpy(np.array(dones)).to(self.device)\n",
    "\n",
    "            # Make a forward pass in the policy network based on the current state\n",
    "            # and choose the q-value of the action\n",
    "#             print(current_state.shape,actions.view(-1,1).shape)\n",
    "            policy_q_values = self.policy_net.forward(current_state).gather(1, actions.view(-1,1))#.view(-1,)\n",
    "            \n",
    "            # Make a forward pass in the target network based on the next state\n",
    "            # and choose the q-value of the action which gives the highest q-value\n",
    "            target_q_values = self.target_net.forward(next_states).max(dim=1).values\n",
    "            \n",
    "            # Compute the target\n",
    "            y_target = list()\n",
    "            for index, value in enumerate(target_q_values):\n",
    "                if dones[i]:\n",
    "                    y_target.append(rewards[index])\n",
    "                else:\n",
    "                    y_target.append(rewards[index] + self.discount_factor * value)\n",
    "            \n",
    "            y_target = torch.stack(y_target) # Create the list to torch tensor\n",
    "\n",
    "            # Calculate the MSE loss and perform a backward pass in the policy network\n",
    "            criterion = nn.MSELoss()\n",
    "            loss = criterion(y_target, policy_q_values)\n",
    "            self.policy_net.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            for param in self.policy_net.parameters():\n",
    "                param.grad.data.clamp(-1, 1)\n",
    "\n",
    "            self.policy_net.optimizer.step()\n",
    "    \n",
    "    def test_model(self, env, episodes, perform_render=False):\n",
    "        '''\n",
    "        Test the agent behavior after it has been trained\n",
    "        Agent follows only the greedy policy\n",
    "        '''\n",
    "        \n",
    "        print('\\nTesting the agent after it has been trained')\n",
    "        print('Agent chooses only greedy actions from the learnt policy')\n",
    "        \n",
    "        total_reward_arr = list()\n",
    "        timesteps = 0\n",
    "        \n",
    "        for episode in range(episodes):\n",
    "            current_state = env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            \n",
    "            while not done:\n",
    "                action = self.get_action(current_state)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "\n",
    "                total_reward += reward\n",
    "                current_state = next_state.copy()\n",
    "                \n",
    "                # Render only for the final episode\n",
    "                if perform_render and episodes-1 == episode:\n",
    "                    timesteps += 1                     \n",
    "                    print('Timestep: ', timesteps)\n",
    "                    env.render()\n",
    "            \n",
    "            total_reward_arr.append(total_reward)\n",
    "        return total_reward_arr\n",
    "    \n",
    "    def save_model(self, filename):\n",
    "        '''\n",
    "        Save the trained model\n",
    "        '''\n",
    "        self.policy_net.save_model(filename)\n",
    "    \n",
    "    def load_model(self, filename):\n",
    "        '''\n",
    "        Load the model\n",
    "        '''\n",
    "        self.policy_net.load_model(filename=filename, device=self.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a67a541-0d8b-4b65-8154-837b4c2e278c",
   "metadata": {},
   "source": [
    "## 4. Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bad293c-925f-48ac-94b5-41144230e857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(title, total_reward, epsilon_decay, cumulative_reward, percentage_success, percentage_failure, avg_timestep, episodes):\n",
    "    '''\n",
    "    This functions plots the graphs for rewards, epsilon, success, failure and avergae timesteps\n",
    "    \n",
    "    Success is defined when the agent has reached the goal state\n",
    "    Failure is defined when the agent meets the monster or falls in the pit\n",
    "    '''\n",
    "    fig, axs = plt.subplots(3,2, figsize=(12,17))\n",
    "    fig.suptitle(title, fontsize=18)\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.91, hspace=0.2, wspace=0.2)\n",
    "    axs[0,0].plot(epsilon_decay)\n",
    "    axs[0,0].set_title('Epsilon Decay')\n",
    "    axs[0,0].set_xlabel('Episodes')\n",
    "    axs[0,0].set_ylabel('Epsilon')\n",
    "    \n",
    "    axs[0,1].plot(avg_timestep)\n",
    "    axs[0,1].set_title('Average timesteps per 50 episodes')\n",
    "    axs[0,1].set_xticks(np.arange(len(avg_timestep)))\n",
    "    axs[0,1].set_xticklabels(list(range(0,episodes+1,50))[1:], rotation=45)\n",
    "    axs[0,1].set_xlabel('Episodes')\n",
    "    axs[0,1].set_ylabel('Timesteps')\n",
    "    \n",
    "    axs[1,0].plot(total_reward)\n",
    "    axs[1,0].set_title('Total Reward per episode')\n",
    "    axs[1,0].set_xlabel('Episodes')\n",
    "    axs[1,0].set_ylabel('Rewards')\n",
    "    \n",
    "    axs[1,1].plot(cumulative_reward)\n",
    "    axs[1,1].set_title('Cumulative reward over all episodes')\n",
    "    axs[1,1].set_xlabel('Episodes')\n",
    "    axs[1,1].set_ylabel('Cumulative Rewards')\n",
    "    \n",
    "    axs[2,0].plot(percentage_success)\n",
    "    axs[2,0].set_title('Percentage of success per 100 episodes')\n",
    "    axs[2,0].set_xticks(np.arange(len(percentage_success)))\n",
    "    axs[2,0].set_xticklabels(list(range(0,episodes+1,100))[1:])\n",
    "    axs[2,0].set_xlabel('Episodes')\n",
    "    axs[2,0].set_ylabel('Success Rate')\n",
    "    \n",
    "    axs[2,1].plot(percentage_failure)\n",
    "    axs[2,1].set_title('Percentage of failure per 100 episodes')\n",
    "    axs[2,1].set_xticks(np.arange(len(percentage_failure)))\n",
    "    axs[2,1].set_xticklabels(list(range(0,episodes+1,100))[1:])\n",
    "    axs[2,1].set_xlabel('Episodes')\n",
    "    axs[2,1].set_ylabel('Failure Rate')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780595b5-112c-4119-89eb-c488140c41e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function where we loop through each episode\n",
    "# and make the agent explore and then exploit the environment\n",
    "# to learn the optimal policy\n",
    "\n",
    "def main(state_size, action_size, epsilon, epsilon_decay, batch_size, lr, discount_factor, memory_len, update_frequency, episodes, test_episodes, perform_render):\n",
    "    device = 'cpu'\n",
    "    total_reward_arr = list() # Store total reward per episode values\n",
    "    cumulative_reward_arr = list() # Store cumulative reward\n",
    "    epsilon_decay_arr = [epsilon] # Store each epsilon value after decay\n",
    "    percentage_success_arr = list() # Store the percentage of success per 100 episodes\n",
    "    percentage_failure_arr = list() # Store the percentage of failure per 100 episodes\n",
    "    timestep_arr = list() # Store the timestep per episode\n",
    "    avg_timestep_arr = list() # Store average timestep every 20 episodes\n",
    "\n",
    "    agent = DQNagent(state_size, action_size, epsilon, epsilon_decay, batch_size, lr, discount_factor, memory_len)\n",
    "    success_count = 0\n",
    "    failure_count = 0\n",
    "\n",
    "    for episode in tqdm(range(episodes)):\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        timesteps = 0\n",
    "        env = gym.make('CartPole-v1')\n",
    "#         env = GridEnvironment()\n",
    "        current_state = env.reset().copy()\n",
    "#         current_state = np.reshape(current_state, [1, state_size])\n",
    "        while not done:\n",
    "            action = agent.get_action(current_state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "#             next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "            agent.append_memory(current_state, action, reward, next_state, done)\n",
    "\n",
    "            agent.train_model()\n",
    "\n",
    "            if episode % update_frequency == 0:\n",
    "                agent.update_target_policy()\n",
    "\n",
    "            current_state = next_state.copy()\n",
    "            total_reward += reward\n",
    "            timesteps += 1\n",
    "\n",
    "        epsilon = agent.update_epsilon()\n",
    "        epsilon_decay_arr.append(epsilon)\n",
    "\n",
    "        timestep_arr.append(timesteps)\n",
    "\n",
    "        # Calculate percentage success\n",
    "        if done and reward == 50:\n",
    "            success_count += 1\n",
    "        if (episode+1) % 100 == 0:\n",
    "            percentage_success_arr.append(success_count)\n",
    "            success_count = 0\n",
    "\n",
    "        # Calculate percentage failure\n",
    "        if done and reward == -50:\n",
    "            failure_count += 1\n",
    "        if (episode+1) % 100 == 0:\n",
    "            percentage_failure_arr.append(failure_count)\n",
    "            failure_count = 0\n",
    "\n",
    "        # Calculate average timesteps for 50 timesteps\n",
    "        if done and (episode+1) % 50 == 0:\n",
    "            avg_timestep_arr.append(np.average(timestep_arr))\n",
    "            timestep_arr.clear()\n",
    "\n",
    "        total_reward_arr.append(total_reward)\n",
    "\n",
    "        if len(cumulative_reward_arr) == 0:\n",
    "            cumulative_reward = total_reward\n",
    "        else:\n",
    "            cumulative_reward = cumulative_reward_arr[-1] + total_reward\n",
    "        cumulative_reward_arr.append(cumulative_reward)\n",
    "    \n",
    "    # Test the agent\n",
    "    test_total_reward_arr = agent.test_model(env, 10, perform_render=True)\n",
    "    \n",
    "    return total_reward_arr, epsilon_decay_arr, cumulative_reward_arr, percentage_success_arr, percentage_failure_arr, avg_timestep_arr, test_total_reward_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e393b918-a0a4-42ed-b555-d89667be143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8276f4d9-26e2-4bdf-b60e-52fd928a170d",
   "metadata": {},
   "source": [
    "### 4.1. Replay memory size: 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e68e07-edea-4bba-81a1-deb616c7e908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.01\n",
    "batch_size = 16\n",
    "lr = 0.01\n",
    "discount_factor = 0.5\n",
    "update_frequency = 3\n",
    "memory_len = 128\n",
    "episodes = 100\n",
    "test_episodes = 10\n",
    "perform_render = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c02a24-4dd3-4824-82ee-80f19606e37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward_arr1, epsilon_decay_arr1, cumulative_reward_arr1, percentage_success_arr1, percentage_failure_arr1, avg_timestep_arr1, test_total_reward_arr1 = main(state_size, action_size, epsilon, epsilon_decay, batch_size, lr, discount_factor, memory_len, update_frequency, episodes, test_episodes, perform_render)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceb1a81-07bc-4004-a0cb-945d58206be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot graphs\n",
    "title = 'Deep Q-Learning: Deterministic\\n' + r'lr: {}, $\\epsilon$: {}, $\\gamma$: {}, episodes: {}, epsilon decay rate: {}, memory length: {}'.format(lr, epsilon, discount_factor, episodes, epsilon_decay, memory_len)\n",
    "\n",
    "plot_graphs(title, total_reward_arr1, epsilon_decay_arr1, cumulative_reward_arr1, percentage_success_arr1, percentage_failure_arr1, avg_timestep_arr1, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de69f4b6-9d6d-49de-a971-9ae590ad11b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_total_reward_arr1)\n",
    "plt.title('Test time: Deep Q-Learning (Deterministic) \\nTotal reward per episode')\n",
    "plt.xlabel('Episodes', fontsize=14)\n",
    "plt.ylabel('Reward per episode', fontsize=14)\n",
    "plt.xticks(np.arange(10), np.arange(1,11))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f90833-ac92-471f-a058-8d4bc87a0931",
   "metadata": {},
   "source": [
    "### 4.2. Replay memory size: 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cc7362-236c-4660-8ad6-e4f19fb660df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "state_size = 25\n",
    "action_size = 4\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.01\n",
    "batch_size = 16\n",
    "lr = 0.01\n",
    "discount_factor = 0.5\n",
    "update_frequency = 3\n",
    "memory_len = 500\n",
    "episodes = 1000\n",
    "test_episodes = 10\n",
    "perform_render = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffcc6c2-28e8-4f58-817e-23f8c2a01c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward_arr2, epsilon_decay_arr2, cumulative_reward_arr2, percentage_success_arr2, percentage_failure_arr2, avg_timestep_arr2, test_total_reward_arr2 = main(state_size, action_size, epsilon, epsilon_decay, batch_size, lr, discount_factor, memory_len, update_frequency, episodes, test_episodes, perform_render)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82261e63-71f2-4fba-bdbb-023830e9974d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot graphs\n",
    "title = 'Deep Q-Learning: Deterministic\\n' + r'lr: {}, $\\epsilon$: {}, $\\gamma$: {}, episodes: {}, epsilon decay rate: {}, memory length: {}'.format(lr, epsilon, discount_factor, episodes, epsilon_decay, memory_len)\n",
    "\n",
    "plot_graphs(title, total_reward_arr2, epsilon_decay_arr2, cumulative_reward_arr2, percentage_success_arr2, percentage_failure_arr2, avg_timestep_arr2, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f505c311-95a4-4383-af52-ee9443127824",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_total_reward_arr2)\n",
    "plt.title('Test time: Deep Q-Learning (Deterministic) \\nTotal reward per episode')\n",
    "plt.xlabel('Episodes', fontsize=14)\n",
    "plt.ylabel('Reward per episode', fontsize=14)\n",
    "plt.xticks(np.arange(10), np.arange(1,11))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c012f146-b562-4e07-ae1b-bd09a8c62fea",
   "metadata": {},
   "source": [
    "### 4.3. Replay memory size: 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963f7d43-f9ef-46de-8616-c10240287b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "state_size = 25\n",
    "action_size = 4\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.01\n",
    "batch_size = 16\n",
    "lr = 0.01\n",
    "discount_factor = 0.5\n",
    "update_frequency = 3\n",
    "memory_len = 1000\n",
    "episodes = 1000\n",
    "test_episodes = 10\n",
    "perform_render = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7b551d-1a98-4981-9d61-9d06343daffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward_arr2, epsilon_decay_arr2, cumulative_reward_arr2, percentage_success_arr2, percentage_failure_arr2, avg_timestep_arr2, test_total_reward_arr2 = main(state_size, action_size, epsilon, epsilon_decay, batch_size, lr, discount_factor, memory_len, update_frequency, episodes, test_episodes, perform_render)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4efd3e-bb8c-4d3b-8acb-eb8c5fa2b5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot graphs\n",
    "title = 'Deep Q-Learning: Deterministic\\n' + r'lr: {}, $\\epsilon$: {}, $\\gamma$: {}, episodes: {}, epsilon decay rate: {}, memory length: {}'.format(lr, epsilon, discount_factor, episodes, epsilon_decay, memory_len)\n",
    "\n",
    "plot_graphs(title, total_reward_arr2, epsilon_decay_arr2, cumulative_reward_arr2, percentage_success_arr2, percentage_failure_arr2, avg_timestep_arr2, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207f8426-61c2-4622-b188-61f7dd29be78",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_total_reward_arr2)\n",
    "plt.title('Test time: Deep Q-Learning (Deterministic) \\nTotal reward per episode')\n",
    "plt.xlabel('Episodes', fontsize=14)\n",
    "plt.ylabel('Reward per episode', fontsize=14)\n",
    "plt.xticks(np.arange(10), np.arange(1,11))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5741f8-36d6-4253-83ce-7ba9ed84bc8d",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f070c1d9-52f4-4093-8aa6-29a9dd4f1054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python388jvsc74a57bd040d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
